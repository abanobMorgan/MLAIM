{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31e8289",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7d1b5b4d5835>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "import gensim \n",
    "import keras \n",
    "\n",
    "import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def word2vec(x_train): \n",
    "\n",
    "\n",
    "    w2v_model = gensim.models.Word2Vec(x_train, size=160, window=5, min_count=4, workers=8)\n",
    "\n",
    "    # Retrieve the weights from the model. This is used for initializing the weights\n",
    "    # in a Keras Embedding layer later\n",
    "    w2v_weights = w2v_model.wv.vectors\n",
    "    vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "    print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))\n",
    "    return w2v_weights, vocab_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer_fit(X_trian,vocab_size , OOV_tok='<OoV>' ): \n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token= OOV_tok)\n",
    "    tokenizer.fit_on_texts(X_trian)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210af8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer_transform(X, tokenizer, max_length,padding_type ): \n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    X_sequences = tokenizer.texts_to_sequences(X)\n",
    "    X_padded = pad_sequences(X_sequences, padding= padding_type, maxlen= max_length)\n",
    "    return X_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb26670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_tokeniz_fit(y): \n",
    "\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(y)\n",
    "    return label_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tokeniz_transform(y,label_tokenizer ): \n",
    "    train_labels_seq=np.array(label_tokenizer.texts_to_sequences(y))\n",
    "\n",
    "    return train_labels_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(vocab_size, embedding_size, max_length, w2v_weights ): \n",
    " \n",
    " \n",
    "    model = Sequential([         # embedding layer\n",
    "        Embedding(vocab_size, embedding_size, input_length= max_length, weights=[w2v_weights], trainable=True),\n",
    "\n",
    "        Bidirectional(LSTM(150, recurrent_dropout=0.3,return_sequences=True)),\n",
    "        Bidirectional(LSTM(70, recurrent_dropout=0.3)),\n",
    "\n",
    "      # Classification head\n",
    "      Dense(180, activation=LeakyReLU()),Dropout(.5),\n",
    "      Dense(64, activation='relu'),Dropout(.5),\n",
    "      Dense(19, activation='softmax')    \n",
    "    ]) \n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModel(model):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    model: \n",
    "\n",
    "    this function takes a model and plot it\n",
    "    \"\"\"\n",
    "    keras.utils.vis_utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    df = pd.read_csv(\"../data/balance_data.txt\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    #np.mean(df.text.map(str).apply(len))\n",
    "\n",
    "    max_length = 160\n",
    "    EPOCH = 30\n",
    "    CLASS_NAMES =list(df.dialect.unique())\n",
    "    NUMBER_OF_CLASSES = len(CLASS_NAMES)\n",
    "    trunc_type = 'post' \n",
    "    padding_type = 'post'\n",
    "    OOV_tok = '<OoV>'\n",
    "\n",
    "    X, y  = LogisticRegression.modelPreporcess(df)\n",
    "\n",
    "\n",
    "    X_train,X_val, X_test,y_train, y_val, y_test = LogisticRegression.model_Split(X, y)\n",
    "\n",
    "\n",
    "    w2v_weights, vocab_size, embedding_size= word2vec(X_train)\n",
    "\n",
    "\n",
    "    tokenizer= tokenizer_fit(X_train,vocab_size)\n",
    "\n",
    "    train_padded=  tokenizer_transform(X_train, tokenizer, max_length,padding_type )\n",
    "    val_padded=tokenizer_transform(X_val, tokenizer, max_length,padding_type )\n",
    "    test_padded=tokenizer_transform(X_test, tokenizer, max_length,padding_type )\n",
    "\n",
    "    label_tokenizer=  label_tokeniz_fit(y)\n",
    "\n",
    "    train_labels_seq=label_tokeniz_transform(y_train,label_tokenizer )\n",
    "    val_labels_seq = label_tokeniz_transform(y_val,label_tokenizer )\n",
    "    test_labels_seq = label_tokeniz_transform(y_test,label_tokenizer )\n",
    "\n",
    "    model = create_model(vocab_size, embedding_size, max_length, w2v_weights )\n",
    "    plotModel(model)\n",
    "    print('model ploted')\n",
    "    history = model.fit(x= train_padded,y= train_labels_seq, \n",
    "                     validation_data=(val_padded,val_labels_seq), epochs=EPOCH)\n",
    "\n",
    "\n",
    "    print(\"done\")\n",
    "    filePath= \"../model/lstm\"\n",
    "    LogisticRegression.saveModel(filePath, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
